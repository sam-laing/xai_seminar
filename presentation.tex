\documentclass{beamer}
\usetheme{Boadilla}
\makeatletter
\def\th@mystyle{%
    \normalfont % body font
    \setbeamercolor{block title example}{bg=orange,fg=white}
    \setbeamercolor{block body example}{bg=orange!20,fg=black}
    \def\inserttheoremblockenv{exampleblock}
  }
\makeatother
\theoremstyle{mystyle}
\newtheorem*{remark}{Remark}
\newtheorem*{task}{Task}
\newtheorem*{idea}{Idea:}
\newtheorem*{dood}{}
\usepackage{bbm}
\usepackage{tikz-cd,mathtools}

\newcommand{\A}{\mathbb{A}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\f}{\mathfrak{f}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\g}{\mathfrak{g}}
\newcommand{\K}{\mathbb{K}}
\renewcommand{\l}{\mathfrak{l}}
\newcommand{\p}{\mathfrak{p}}
\renewcommand{\P}{\mathfrak{P}}
\newcommand{\PP}{\mathbb{P}}
\mode<presentation>{}
\usepackage{amscd}
\tikzcdset{ampersand replacement=\&}
\usepackage{tikz-cd}
\tikzcdset{ampersand replacement=\&}
\usepackage{tikz}
\usepackage{mathtools}
\usepackage{array}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage[english]{babel}
\usepackage{amsmath, amssymb}
\usepackage[mathscr]{euscript}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{xcolor}
\graphicspath{ {./} }
\begin{document}
\title{A Benchmark for Interpretability Methods in DNNs}
\subtitle{(Google Brain)}
\author{Sam Laing }
\institute{University of Tuebingen}
\date{\today}
\begin{frame}
\titlepage
\end{frame}
\begin{frame}
	\frametitle{A Bit of Background}
	\includegraphics[width = 11cm, height = 5cm]{dog_net2.png} \pause
	\begin{itemize}
		\item Deep image classification: "features" = pixels .\pause
		\item Interpretability methods $\to$ help engineer understand their model\pause
		\item Austensibly. But are they even right?
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Included Interpretability Methods}
	\includegraphics[width=10cm, height=5cm]{compareSGBP.png}

	\begin{itemize}
	
	\item Gradients (sensitivity heatmaps) $e = \partial_{x_i} A_n^{\ell} $ \pause

	\item Guided Backprop (sort of a tidied up sensitivity map). Keep positives in ReLU \pause
	\item Integrated Gradients
	
	\end{itemize}
\end{frame}
\begin{frame}
	\frametitle{Included Interpretability Methods: Ensembling in a Nutshell}
	\includegraphics[width=10cm, height=4.5cm]{compareSGBP.png}
\begin{itemize}
\item Inject inputs with Gaussian noise consider mean/variance of outputs \pause
\item $\eta_i \sim N(0, \sigma ^2 I)$ $i\in \{1,\ldots, J\} $ \pause
\item SmoothGrad (SG) $e = \sum_{i=1}^{J} {f( x + \eta_i, A_n^{\ell}) }$ \pause
\item VarGrad (VAR) $e = \text{Var}\left(  \sum_{i}^{J} {f(x + \eta_i, A_n^{\ell})}\right) $ \pause
\item SmoothGrad Squared (SG-SQ) $e = \sum_{i=1}^{J} {f( x + \eta_i, A_n^{\ell})^{2} }$
\end{itemize}
\end{frame}
\begin{frame}
	\frametitle{Motivation}
	\begin{itemize}
		\item Is my interpretability method is really doing anything? \pause
		\item Different methods may consider different features important \pause
		\item Interpretability method A $>$ Interpretability method B??\pause
		\item If only there was a benchmarking framework to do this...
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{ROAR (RemOve And Retrain)}
	\includegraphics[height=6cm, width=8cm]{tiger.png}

	For each interpretability method, sort pixels by ranked importance. \\ \pause 
	

	So $(e_j)_{j=1}^{D}$ of pixel coordinates $\forall $ image in dataset. $\implies ( (e_j^{(n)})_{j=1}^D )_{n=1}^N $
\end{frame}
\begin{frame}
	\frametitle{The idea behind ROAR}

	for $j \in \{0, 10, \ldots,100\} $, replace the top j\% ranked pixels with the per channel mean  $\forall $ image and retrain.
	\includegraphics[height=8cm, width=10cm]{tiger0.1.png}

\end{frame}
\begin{frame}
	\frametitle{The idea behind ROAR}
	\includegraphics[height=8cm, width=10cm]{tiger0.3.png}

\end{frame}

\begin{frame}
	\frametitle{The idea behind ROAR}
	\includegraphics[height=8cm, width=10cm]{tiger0.5.png}

\end{frame}

\begin{frame}
	\frametitle{The idea behind ROAR}
	\includegraphics[height=8cm, width=10cm]{tiger0.7.png}

\end{frame}


\begin{frame}
	\frametitle{The idea behind ROAR}
	\includegraphics[height=8cm, width=10cm]{tiger0.9.png}

\end{frame}



\begin{frame}
	\frametitle{The idea behind ROAR}
	\begin{itemize}
		\item Effect of having dropped the "most informative pixels" as determined by each interpretability method.
		\item Investigate how much their removal from the training process effects accuracy.		\item Also a no-retraining variant


	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{To Retrain Or not To Retrain}
	\begin{itemize}
	\item Without retraining the model, train and test come from different distributions... violates key assumption of ML \pause
	\item Paper therefore argues it is necessary \pause
	\end{itemize}
	\includegraphics[width=12cm, height=5cm]{retrain_vs_not.png}
\end{frame}
\begin{frame}
	\frametitle{An Outline of the Experiment}
	Really just a refinement of above.
	\begin{itemize}
		\item ResNet50 classifier: Imagenet, Birdsnap and Food 101\pause
		\item Along with a number of different interpretability methods, a random ranking was also included: This tells us if the method outperforms random.\pause
		\item  New train and test sets are generated for each $j \in \{0,10,30,50,70,90\} $\pause
		\item For fairness, the model is retrained 5 times for each method (DNN training is noisy) \pause
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{ROAR in Action}
	\includegraphics[height=8cm, width=10cm]{ROAR_methods.png}
\end{frame}
\begin{frame}
	\frametitle{Results}
	\begin{itemize}
		\item Surprisingly, replacing large numbers of pixels doesn't remove that much predictive power! \pause
		\item For ImageNet, after 90\% of the pixels are randomly removed, still 63.53\% accuracy relative to the original 78.68\% \pause
		\item Don't worry... this paper is from 2018, they don't stink at training networks :)\pause
		\item According to the paper, SG-SQ and VarGrad are the real heros \pause
	\end{itemize} 
	\includegraphics[width=11cm, height=4cm]{ImgNetSG.png}

\end{frame}
\begin{frame}
	\includegraphics[width=11cm, height=8cm]{other2SG.png}
\end{frame}
\begin{frame}
	\frametitle{Results (cont.)}
	\begin{itemize}
	\item The dataset affects which 
	\end{itemize}
\end{frame}
\begin{frame}
    \frametitle{A Few Possible Issues in the Approach}
    \begin{itemize}
        \item This experiment replaces the top j pixels with the mean of the image.
        \item Is this really the best way?
        \item The mean still conveys possibly useful information
        \item Alternative methods are sometimes advised but most have their own issues.
    \end{itemize}
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth, height=3.5cm]{black_car.png}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth, height=3.5cm]{green_frog.png}
    \end{minipage}
\end{frame}
\begin{frame}
	\frametitle{Another possible Issue}
	\begin{itemize}
	\item In practice retraining a large image classifier several times is pretty unfeasible computationally speaking.  (ImageNet with ResNet50 can take 
	\item Without retraining, you run into theoretical violations of ML!
	\end{itemize}
\end{frame}A
\begin{frame}
	\frametitle{Yet Another Possible Issue}

\end{frame}
\begin{frame}
	\frametitle{Why I chose this paper}
\end{frame}
\begin{frame}
	\frametitle{Questions?}
\end{frame}
\end{document}

